#include <cstdint>
#include <cstdlib>
#include <filesystem>
#include <iostream>

#include <pyc/cpp/pyc_tb.hpp>

// Generated by `pyc-compile --emit=cpp`.
#include "janus_cube_pyc_gen.hpp"

using pyc::cpp::Testbench;
using pyc::cpp::Wire;

namespace {

// Memory-mapped addresses (must match cube_v2_consts.py)
constexpr std::uint64_t kBaseAddr = 0x80000000ull;
constexpr std::uint64_t kAddrControl = kBaseAddr + 0x0000;
constexpr std::uint64_t kAddrStatus = kBaseAddr + 0x0008;
constexpr std::uint64_t kAddrMatmulInst = kBaseAddr + 0x0010;

// Control bits
constexpr std::uint64_t kCtrlStart = 1 << 0;
constexpr std::uint64_t kCtrlReset = 1 << 1;

// Status bits
constexpr std::uint64_t kStatDone = 1 << 0;
constexpr std::uint64_t kStatBusy = 1 << 1;

// Helper to write to memory-mapped register
static void mmioWrite(pyc::gen::janus_cube_pyc &dut, std::uint64_t addr, std::uint64_t data) {
  dut.mem_wvalid = Wire<1>(1);
  dut.mem_waddr = Wire<64>(addr);
  dut.mem_wdata = Wire<64>(data);
}

// Helper to clear write signals
static void mmioWriteClear(pyc::gen::janus_cube_pyc &dut) {
  dut.mem_wvalid = Wire<1>(0);
  dut.mem_waddr = Wire<64>(0);
  dut.mem_wdata = Wire<64>(0);
}

// Test: Measure compute cycles for MATMUL instruction
static bool testComputeCycles(int M, int K, int N) {
  std::cout << "\n=== Testing " << M << "x" << K << "x" << N << " MATMUL ===\n";

  pyc::gen::janus_cube_pyc dut{};
  Testbench<pyc::gen::janus_cube_pyc> tb(dut);

  tb.addClock(dut.clk, /*halfPeriodSteps=*/1);
  tb.reset(dut.rst, /*cyclesAsserted=*/2, /*cyclesDeasserted=*/1);

  // Clear any previous state
  mmioWrite(dut, kAddrControl, kCtrlReset);
  tb.runCycles(1);
  mmioWriteClear(dut);
  tb.runCycles(5);

  // Write MATMUL instruction (M, K, N packed into 64 bits)
  // Format: [15:0]=M, [31:16]=K, [47:32]=N
  std::uint64_t inst = (static_cast<std::uint64_t>(M) & 0xFFFF) |
                       ((static_cast<std::uint64_t>(K) & 0xFFFF) << 16) |
                       ((static_cast<std::uint64_t>(N) & 0xFFFF) << 32);
  mmioWrite(dut, kAddrMatmulInst, inst);
  tb.runCycles(1);
  mmioWriteClear(dut);
  tb.runCycles(1);

  // Start computation
  mmioWrite(dut, kAddrControl, kCtrlStart);
  tb.runCycles(1);
  mmioWriteClear(dut);

  // Wait for done signal
  int cycles = 0;
  int timeout = 100000;
  while (!dut.done.toBool() && timeout > 0) {
    tb.runCycles(1);
    cycles++;
    timeout--;
  }

  if (timeout == 0) {
    std::cerr << "TIMEOUT after " << cycles << " cycles!\n";
    return false;
  }

  std::cout << "Computation completed in " << cycles << " cycles\n";

  // Calculate theoretical cycles
  // Assuming 16x16 PE array (default config)
  int tile_size = 16;
  int m_tiles = (M + tile_size - 1) / tile_size;
  int k_tiles = (K + tile_size - 1) / tile_size;
  int n_tiles = (N + tile_size - 1) / tile_size;
  int total_uops = m_tiles * k_tiles * n_tiles;
  int pipeline_latency = 4;
  int theoretical = total_uops + pipeline_latency - 1;

  std::cout << "Theoretical: " << theoretical << " cycles "
            << "(tiles: " << m_tiles << "x" << k_tiles << "x" << n_tiles
            << " = " << total_uops << " uops)\n";

  return true;
}

} // namespace

int main(int argc, char **argv) {
  std::cout << "Cube Accelerator Cycle Count Test\n";
  std::cout << "==================================\n";

  // Test various matrix sizes
  bool ok = true;

  // Small test first
  ok = ok && testComputeCycles(16, 16, 16);

  // 32x32x32
  ok = ok && testComputeCycles(32, 32, 32);

  // 64x64x64
  ok = ok && testComputeCycles(64, 64, 64);

  if (ok) {
    std::cout << "\nAll tests passed!\n";
    return 0;
  } else {
    std::cerr << "\nSome tests failed!\n";
    return 1;
  }
}
